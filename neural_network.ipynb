{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJ8p1HDsmHUsq+f2t+4h30"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "df=pd.read_csv('gdrive/My Drive/emnist-letters-train.csv')\n",
        "df1=pd.read_csv('gdrive/My Drive/emnist-letters-test.csv')\n",
        "X_train=df.values[:, 1:785]\n",
        "y_train=df.values[:, 0]\n",
        "X_test =df1.values[:, 1:785]\n",
        "y_test =df1.values[:, 0]\n",
        "m=len(y_train)\n",
        "\n",
        "\n",
        "class MLP(object):\n",
        "  def __init__(self, eta=0.001, alpha=0.001, n_iter=50, batch_size=100):\n",
        "        self.eta = eta\n",
        "        self.alpha = alpha\n",
        "        self.n_iter = n_iter\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "       \n",
        "\n",
        "  def one_hot_enc(self, y, num_labels=27):\n",
        "    one_hot = np.zeros((num_labels, y.shape[0]), dtype=np.float32)\n",
        "    \n",
        "    for i, val in enumerate(y):\n",
        "      one_hot[val,i] = 1.0\n",
        "    \n",
        "    return one_hot\n",
        "\n",
        "  def init_weights(self, n_input, n_hidden_1, n_hidden_2, n_output):\n",
        "    w1 = np.random.randn(n_hidden_1, n_input+1)\n",
        "    w2 = np.random.randn(n_hidden_2, n_hidden_1+1)\n",
        "    w3 = np.random.randn(n_output, n_hidden_2+1)\n",
        "        \n",
        "    return w1, w2, w3 \n",
        "\n",
        "  def add_bias_unit(self, layer, orientation):\n",
        "    if orientation == 'row':\n",
        "      updated_layer = np.ones((layer.shape[0]+1, layer.shape[1]))\n",
        "      updated_layer[1:, :] = layer\n",
        "    elif orientation == 'col':\n",
        "      updated_layer = np.ones((layer.shape[0], layer.shape[1] + 1))\n",
        "      updated_layer[:, 1:] = layer\n",
        "    \n",
        "    return updated_layer \n",
        "\n",
        "\n",
        "  def compute_forward_pass(self, input):\n",
        "    a1 = self.add_bias_unit(input, orientation='col')\n",
        "    \n",
        "    z2 = np.matmul(self.w1, a1.transpose(1, 0))\n",
        "    a2 = 1/(1 + np.exp(-z2))\n",
        "    a2 = self.add_bias_unit(a2, orientation='row')\n",
        "    \n",
        "    z3 = np.matmul(self.w2, a2)\n",
        "    a3 = 1/(1 + np.exp(-z3))\n",
        "    a3 = self.add_bias_unit(a3, orientation='row')\n",
        "    \n",
        "    z4 = np.matmul(self.w3, a3)\n",
        "    a4 = 1/(1 + np.exp(-z4))\n",
        "    \n",
        "    return a1, z2, a2, z3, a3, z4, a4\n",
        "\n",
        "\n",
        "  def predict(self, a4):\n",
        "    prediction = np.argmax(a4, axis=0)\n",
        "    return prediction\n",
        "\n",
        "  def compute_loss(self, prediction, label):\n",
        "    term_1 = -1*label * np.log(prediction)\n",
        "    term_2 = (1-label)*(np.log(1-prediction))\n",
        "    \n",
        "    loss = np.sum(term_1 - term_2)\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def compute_backward_pass(self, outputs, label):\n",
        "    a1, z2, a2, z3, a3, z4, a4 = outputs\n",
        "    \n",
        "    delta_4 = a4 - label\n",
        "    sig_z3 = np.array(1/(1 + np.exp(-z3)))\n",
        "    delta_3 = np.matmul(self.w3[:,1:].transpose(),delta_4)*sig_z3*(1-sig_z3)\n",
        "               \n",
        "    sig_z2 = np.array(1/(1 + np.exp(-z2)))\n",
        "    delta_2 = np.matmul(self.w2[:,1:], delta_3)*(sig_z2)*(1-(sig_z2))\n",
        "    \n",
        "    grad_w1 = np.matmul(delta_2, a1)\n",
        "    grad_w2 = np.matmul(delta_3, a2.transpose())\n",
        "    grad_w3 = np.matmul(delta_4, a3.transpose())\n",
        "    \n",
        "    return grad_w1, grad_w2, grad_w3\n",
        "  def fit(self, X, y):\n",
        "        n_input = len(X[0,0,:]) #returns the flattened image size (28*28 = 784)\n",
        "    \n",
        "        n_hidden_1, n_hidden_2, n_output = 100, 100, 27\n",
        "        self.w1, self.w2, self.w3 = self.init_weights(n_input, n_hidden_1, n_hidden_2,\n",
        "                                    n_output)\n",
        "\n",
        "        delta_w1_prev = np.zeros(self.w1.shape)\n",
        "        delta_w2_prev = np.zeros(self.w2.shape)\n",
        "        delta_w3_prev = np.zeros(self.w3.shape)\n",
        "        \n",
        "        train_losses = []\n",
        "        train_acc = []\n",
        "    \n",
        "        c=0\n",
        "        for i in range(self.n_iter):\n",
        "            for j, (input, label) in enumerate(zip(X, y)):\n",
        "                one_hot_label = self.one_hot_enc(label, num_labels=27)\n",
        "                \n",
        "                a1, z2, a2, z3, a3, z4, a4 = self.compute_forward_pass(input)\n",
        "                loss = self.compute_loss(a4, one_hot_label)\n",
        "                grad1, grad2, grad3 = self.compute_backward_pass([a1, z2, a2, z3, a3, z4, a4],\n",
        "                                                one_hot_label)\n",
        "    \n",
        "                delta_w1, delta_w2, delta_w3 = self.eta*grad1, self.eta*grad2, self.eta*grad3\n",
        "    \n",
        "                self.w1 -= delta_w1 + delta_w1_prev*self.alpha\n",
        "                self.w2 -= delta_w2 + delta_w2_prev*self.alpha\n",
        "                self.w3 -= delta_w3 + delta_w3_prev*self.alpha\n",
        "    \n",
        "                delta_w1_prev, delta_w2_prev, delta_w3_prev = delta_w1, delta_w2, delta_w3\n",
        "    \n",
        "                train_losses.append(loss)\n",
        "                predictions = self.predict(a4)\n",
        "                #print(label)\n",
        "                wrong = np.where(predictions != label,\n",
        "                                np.matrix([1.]), np.matrix([0.]))\n",
        "                if i==49:\n",
        "                  for l in range(100):\n",
        "                    if predictions[l]==label[l]:\n",
        "                      c=c+1\n",
        "                accuracy = 1 - np.sum(wrong)/self.batch_size\n",
        "    \n",
        "                train_acc.append(accuracy)\n",
        "                \n",
        "        '''print(self.n_iter-1, 'training accuracy %.2f' %\n",
        "                    np.mean(np.matrix(train_acc)).item())'''\n",
        "        return c     \n",
        "\n",
        "\n",
        "  def norm(self, X, x_min, x_max):\n",
        "        nom = (X-X.min(axis=0))*(x_max-x_min)\n",
        "        denom = X.max(axis=0) - X.min(axis=0)\n",
        "        denom[denom==0] = 1\n",
        "        return x_min + nom/denom \n",
        "    \n",
        "  def prep_data(self, X, y):\n",
        "    X_ = []\n",
        "    y_ = []\n",
        "    itr = int(len(y)/self.batch_size)+1\n",
        "    for j in range(1,itr):\n",
        "      rng = j*self.batch_size\n",
        "      X_.append(X[rng-self.batch_size:rng, :])\n",
        "      y_.append(y[rng-self.batch_size:rng])\n",
        "        \n",
        "    X, y = np.array(X_), np.array(y_)\n",
        "    X = self.norm(X, 0, 1)\n",
        "        \n",
        "    return X, y\n",
        "\n",
        "mlp=MLP()\n",
        "X_train, y_train = mlp.prep_data(X_train, y_train)\n",
        "c=mlp.fit(X_train, y_train)\n",
        "\n",
        "p=c/m\n",
        "print(p*100,\"percent of the predicted values are correct\")    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP7vbOqgr18j",
        "outputId": "95be5f62-1e37-4b2d-c549-60da4b694766"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "76.74185520107208 percent of the predicted values are correct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6SRArWTrr3b1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}